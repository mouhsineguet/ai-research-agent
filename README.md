# AI Research Assistant for Technical Literature Review

An intelligent multi-agent system designed to streamline technical literature review by automating research tasks, managing information overload, and providing structured insights through AI-powered analysis.

## ğŸ¯ Project Overview

This AI Research Assistant is a sophisticated system that helps researchers navigate and analyze technical literature efficiently. It employs a multi-agent architecture to:
- Process user research questions
- Search across multiple academic APIs (ArXiv, PubMed, Semantic Scholar)
- Generate comprehensive paper summaries and critiques
- Suggest research directions and next steps
- Maintain context through persistent memory
- Incorporate human feedback in decision-making
- Provide traceable and evaluable research processes

## ğŸ§  Why This Project Matters

- **Solves Real Problems**: Addresses information overload in research by automating literature review tasks
- **Advanced Architecture**: Implements cutting-edge AI concepts including:
  - Large Language Models (LLMs)
  - Persistent Memory Systems
  - Tool Integration
  - Evaluation Frameworks
  - Human-in-the-Loop Design
- **Technical Excellence**: Demonstrates deep understanding of:
  - LangGraph for agent orchestration
  - Agentic patterns and workflows
  - Modern AI system design

## ğŸ› ï¸ Key Features

| Feature | Technology / Methodology |
|---------|-------------------------|
| Multi-agent Coordination | LangGraph nodes, state machine logic |
| Supervisor & Worker Agents | Agentic patterns (Planner, Critique, Supervisor) |
| Tool Integration | ArXiv, PubMed, Semantic Scholar APIs |
| Memory Management | LangGraph with LangChain ConversationBuffer |
| Human-in-the-Loop | Interactive feedback and confirmation |
| Evaluation & Tracing | LangSmith integration for monitoring and scoring |

## ğŸ“‹ Project Architecture

```
User Input
   |
   v
[Planner Agent] ---> Decides which agents/tools to trigger
   |
   +---> [Tool Agent] --> Search APIs, retrieve metadata & abstracts
   |
   +---> [Summarizer Agent] --> Summarizes relevant papers
   |
   +---> [Critique Agent] --> Checks if answer quality is sufficient
   |
   +---> [Memory Manager] --> Persists session history, user preferences
   |
   v
[Supervisor Agent]
   |
   +---> If output is OK --> Done
   +---> If unclear --> Ask human for input (Human-in-the-loop)
   |
   v
[Output + LangSmith Trace + Score]
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ README.md
â”œâ”€â”€ main.py                 # Main entry point and agent orchestration
â”œâ”€â”€ graph/                  # Agent definitions and orchestration
â”‚   â”œâ”€â”€ planner_agent.py    # Research strategy and task planning
â”‚   â”œâ”€â”€ summarizer_agent.py # Paper summarization and analysis
â”‚   â”œâ”€â”€ tool_agent.py       # API integration and tool management
â”‚   â”œâ”€â”€ critique_agent.py   # Quality assessment and validation
â”‚   â””â”€â”€ supervisor_agent.py # Agent coordination and human interaction
â”œâ”€â”€ tools/                  # External API integrations
â”‚   â””â”€â”€ arxiv_api.py        # ArXiv API client
â”œâ”€â”€ memory/                 # State and context management
â”‚   â””â”€â”€ memory_manager.py   # Persistent memory implementation
â”œâ”€â”€ evaluation/            # Monitoring and evaluation
â”‚   â””â”€â”€ langsmith_config.py # LangSmith integration
â”œâ”€â”€ .env                   # Environment configuration
â””â”€â”€ requirements.txt       # Project dependencies
```

## ğŸš€ Getting Started

1. Clone the repository
2. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
4. Configure environment variables:
   - Copy `.env.example` to `.env`
   - Add your API keys for:
     - OpenAI
     - LangSmith
     - ArXiv
     - PubMed
     - Semantic Scholar

## ğŸ’¡ Usage

1. Start the research assistant:
   ```bash
   python main.py
   ```
2. Enter your research question
3. Review and provide feedback on:
   - Search results
   - Paper summaries
   - Research suggestions
4. Access evaluation metrics through LangSmith

## ğŸ”„ Workflow

1. **Input Processing**: User research question is received and parsed
2. **Planning**: Planner agent determines research strategy
3. **Execution**: Tool agent searches APIs for relevant papers
4. **Analysis**: Summarizer and Critique agents process papers
5. **Review**: Supervisor agent coordinates with human feedback
6. **Output**: Final research summary with next steps
7. **Evaluation**: LangSmith traces and scores the process

## ğŸ“Š Evaluation

The system uses LangSmith for:
- Tracing agent interactions
- Collecting human feedback
- Scoring response quality
- Monitoring system performance
- Identifying improvement areas

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

MIT License - See LICENSE file for details 